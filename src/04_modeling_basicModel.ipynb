{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import shared_functions as sf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path and file name\n",
    "import_path = '../data/processed/'\n",
    "file_name = 'property-sales_new-york-city_2022_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name\n",
    "model_name = 'basicModel'\n",
    "\n",
    "# Creating output directory for exports\n",
    "Path(f'../models/{model_name}').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: CPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "print(f'Device type: {device.upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(f'{import_path}{file_name}.parquet').iloc[:, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output arrays from dataset\n",
    "y = df.sale_price\n",
    "X = df.drop(columns=y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array       Length (n)  Length (%)\n",
      "----------------------------------\n",
      "training         18589      70.00%\n",
      "validation        2655      10.00%\n",
      "testing           5310      20.00%\n",
      "----------------------------------\n",
      "total            26554     100.00%\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation and testing sets\n",
    "# make this more agnostic\n",
    "X_split, y_split = sf.train_test_val_dict(X, y, val_size=0.1, test_size=0.2, shuffle=True, random_state=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize input arrays (this also ensures all values are numeric)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_split['train'])\n",
    "X_scaled = {key:scaler.transform(X_array) for key, X_array in zip(dataset_labels, [X_split[key] for key in X_split])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X tensors\n",
    "X_tensors = {key:torch.Tensor(X_array) for key, X_array in zip(dataset_labels, [X_scaled[key] for key in X_scaled])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor datasets\n",
    "tensor_datasets = {key:torch.utils.data.TensorDataset(X_array, torch.Tensor(y_array.values)) for key, X_array, y_array in zip(dataset_labels, [X_tensors[key] for key in X_tensors], [y_split[key] for key in y_split])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class basicModel(nn.Module):\n",
    "    # Define model components\n",
    "    def __init__(self):\n",
    "        # Inherit from parent class\n",
    "        super(basicModel, self).__init__()\n",
    "\n",
    "        # Define linear layers\n",
    "        # self.linear1 = nn.Linear(8, 200)\n",
    "        # self.linear2 = nn.Linear(200, 100)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "        # Define acitvation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    # Define forward pass\n",
    "    def forward(self, X):\n",
    "        # X = self.relu(self.linear1(X))\n",
    "        # X = self.relu(self.linear2(X))\n",
    "        y = self.relu(self.linear3(X))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = basicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model paramters: 9\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of model parameters\n",
    "n_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "print(f'# model paramters: {n_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basicModel(\n",
       "  (linear3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass model to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_function = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization alogrithm\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data loaders\n",
    "# maybe replace X_split with tensor_dataset or sth\n",
    "dataloaders = {key:torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size if key=='train' else len(X_split[key]), shuffle=False) for key, tensor_dataset in zip(dataset_labels, [tensor_datasets[key] for key in tensor_datasets])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/miniconda3/envs/bachelors_thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/jonas/miniconda3/envs/bachelors_thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/jonas/miniconda3/envs/bachelors_thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/jonas/miniconda3/envs/bachelors_thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/jonas/miniconda3/envs/bachelors_thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Training loss: 299154075465692.8, Validation loss: 4926047518720.0\n",
      "Epoch 1/10, Training loss: 299154062219538.5, Validation loss: 4926042800128.0\n",
      "Epoch 2/10, Training loss: 299154050623269.8, Validation loss: 4926038081536.0\n",
      "Epoch 3/10, Training loss: 299154039235545.3, Validation loss: 4926033362944.0\n",
      "Epoch 4/10, Training loss: 299154029332177.4, Validation loss: 4926028120064.0\n",
      "Epoch 5/10, Training loss: 299154015889415.06, Validation loss: 4926022877184.0\n",
      "Epoch 6/10, Training loss: 299154005436085.25, Validation loss: 4926018158592.0\n",
      "Epoch 7/10, Training loss: 299153997106932.56, Validation loss: 4926012915712.0\n",
      "Epoch 8/10, Training loss: 299153985989684.8, Validation loss: 4926008197120.0\n",
      "Epoch 9/10, Training loss: 299153977014630.94, Validation loss: 4926002954240.0\n",
      "Epoch 10/10, Training loss: 299153964991590.06, Validation loss: 4925998235648.0\n"
     ]
    }
   ],
   "source": [
    "history = pd.DataFrame({\n",
    "    'loss_train': np.nan,\n",
    "    'loss_val': np.nan\n",
    "    }, index=pd.Index(np.arange(epochs) + 1, name='epoch'))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs + 1):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    epoch_loss_train = []\n",
    "\n",
    "    for i, (batch_X, batch_y) in enumerate(dataloaders['train']):\n",
    "        # Pass batch to GPU\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output_train = model(batch_X)\n",
    "        model.zero_grad()\n",
    "        batch_loss_train = loss_function(output_train, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_train.append(batch_loss_train.data.item())\n",
    "    \n",
    "    # Calculate training loss\n",
    "    epoch_loss_train = np.mean(epoch_loss_train)\n",
    "    history.loss_train.loc[epoch] = epoch_loss_train.item()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    model.eval() # deactivates potential Dropout and BatchNorm\n",
    "    with torch.no_grad():\n",
    "        output_val = model(batch_X) # this cannot also be batch_X needs to be dataloader['val'] or sth\n",
    "        epoch_loss_val = loss_function(output_val, batch_y)\n",
    "        history.loss_val.loc[epoch] = epoch_loss_val.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}/{epochs}, Training loss: {epoch_loss_train.item()}, Validation loss: {epoch_loss_val.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_csv(f'../models/{model_name}/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model predictions\n",
    "predictions = sf.get_predictions(model, X_tensors, y_split, dataset_labels, save_as=f'../models/{model_name}/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      CPE          RMSE          MAE      R2\n",
      "---------------------------------------------------\n",
      "train      1.000  17312063.083  2159515.885  -0.016\n",
      "val        1.000  10087510.242  1999090.908  -0.041\n",
      "test       1.000  19788940.085  2226796.547  -0.013\n"
     ]
    }
   ],
   "source": [
    "# Compute performance metrics\n",
    "metrics = sf.calc_metrics(predictions, save_as=f'../models/{model_name}/metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelors_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
