{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import shared_functions as sf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path and file name\n",
    "import_path = '../data/processed/'\n",
    "file_name = 'property-sales_new-york-city_2022_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name\n",
    "model_path = '../models/'\n",
    "model_name = 'basicModel'\n",
    "\n",
    "# Creating output directory for exports\n",
    "Path(f'{model_path}{model_name}').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: CPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "print(f'Device type: {device.upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(f'{import_path}{file_name}.parquet').iloc[:, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output arrays from dataset\n",
    "y = df.sale_price\n",
    "X = df.drop(columns=y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset  Size (n)  Size (%)\n",
      "--------------------------\n",
      "train      18589    70.00%\n",
      "val         2655    10.00%\n",
      "test        5310    20.00%\n",
      "--------------------------\n",
      "total      26554   100.00%\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation and testing sets\n",
    "X_split, y_split, subset_keys = sf.train_test_val_dict(X, y, val_size=0.1, test_size=0.2, shuffle=True, random_state=42, save_subset_sizes_as='../data/processed/subset_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize input arrays (this also ensures all values are numeric)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_split['train'])\n",
    "X_scaled = {subset_key:scaler.transform(X_array) for subset_key, X_array in zip(subset_keys, [X_split[subset_key] for subset_key in subset_keys])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize input arrays (this also ensures all values are numeric)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.DataFrame(y_split['train']))\n",
    "y_scaled = {subset_key:scaler.transform(y_array) for subset_key, y_array in zip(subset_keys, [pd.DataFrame(y_split[subset_key]) for subset_key in subset_keys])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X tensors\n",
    "X_tensors = {subset_key:torch.Tensor(X_array) for subset_key, X_array in zip(subset_keys, [X_scaled[subset_key] for subset_key in subset_keys])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor datasets\n",
    "tensor_datasets = {subset_key:torch.utils.data.TensorDataset(X_array, torch.Tensor(y_array)) for subset_key, X_array, y_array in zip(subset_keys, [X_tensors[subset_key] for subset_key in subset_keys], [y_scaled[subset_key] for subset_key in subset_keys])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class basicModel(nn.Module):\n",
    "    # Define model components\n",
    "    def __init__(self):\n",
    "        # Inherit from parent class\n",
    "        super(basicModel, self).__init__()\n",
    "\n",
    "        # Define linear layers\n",
    "        # self.linear1 = nn.Linear(8, 200)\n",
    "        # self.linear2 = nn.Linear(200, 100)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "        # Define acitvation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    # Define forward pass\n",
    "    def forward(self, X):\n",
    "        # X = self.relu(self.linear1(X))\n",
    "        # X = self.relu(self.linear2(X))\n",
    "        y = self.relu(self.linear3(X))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = basicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model paramters: 9\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of model parameters\n",
    "n_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "print(f'# model paramters: {n_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basicModel(\n",
       "  (linear3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass model to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_function = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization alogrithm\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data loaders\n",
    "dataloaders = {subset_key:torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size if subset_key=='train' else len(tensor_datasets[subset_key]), shuffle=False) for subset_key, tensor_dataset in zip(subset_keys, [tensor_datasets[subset_key] for subset_key in subset_keys])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training loss: 0.8952365433397188, Validation loss: 0.00945769902318716\n",
      "Epoch 2/3, Training loss: 0.7363498403748363, Validation loss: 0.007746731862425804\n",
      "Epoch 3/3, Training loss: 0.7032218395389899, Validation loss: 0.0067726243287324905\n"
     ]
    }
   ],
   "source": [
    "history = pd.DataFrame({\n",
    "    'loss_train': np.nan,\n",
    "    'loss_val': np.nan\n",
    "    }, index=pd.Index(np.arange(epochs) + 1, name='epoch'))\n",
    "\n",
    "# Training loop\n",
    "for epoch in np.arange(epochs) + 1:\n",
    "    # Train model\n",
    "    model.train()\n",
    "    epoch_loss_train = []\n",
    "\n",
    "    for i, (batch_X, batch_y) in enumerate(dataloaders['train']):\n",
    "        # Pass batch to GPU\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output_train = model(batch_X)\n",
    "        model.zero_grad()\n",
    "        batch_loss_train = loss_function(output_train, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_train.append(batch_loss_train.data.item())\n",
    "    \n",
    "    # Calculate training loss\n",
    "    epoch_loss_train = np.mean(epoch_loss_train)\n",
    "    history.loss_train.loc[epoch] = epoch_loss_train.item()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    model.eval() # deactivates potential Dropout and BatchNorm\n",
    "    with torch.no_grad():\n",
    "        output_val = model(batch_X) # this cannot also be batch_X needs to be dataloader['val'] or sth\n",
    "        epoch_loss_val = loss_function(output_val, batch_y)\n",
    "        history.loss_val.loc[epoch] = epoch_loss_val.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}/{epochs}, Training loss: {epoch_loss_train.item()}, Validation loss: {epoch_loss_val.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history.to_csv(f'{model_path}{model_name}/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model predictions\n",
    "predictions = sf.get_predictions(model, X_tensors, y_split, subset_keys, save_as=f'{model_path}{model_name}/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset    CPE          RMSE          MAE      R2\n",
      "------------------------------------------------\n",
      "train   1.000  17312078.364  2159542.609  -0.016\n",
      "val     1.000  10087543.662  1999118.037  -0.041\n",
      "test    1.000  19788955.801  2226823.996  -0.013\n",
      "------------------------------------------------\n",
      "total   1.000  17271139.553  2156956.819  -0.016\n"
     ]
    }
   ],
   "source": [
    "# Compute performance metrics\n",
    "metrics = sf.get_metrics(predictions, subset_keys, save_as=f'{model_path}{model_name}/perf_metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelors_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
