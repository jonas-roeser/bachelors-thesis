{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path and file name\n",
    "file_path = '../data/raw/'\n",
    "file_name = 'property-sales_new-york-city_2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(f'{file_path}{file_name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API settings for geocoding\n",
    "geolocator = Nominatim(user_agent='property-sales-locator')\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before: 93427\n",
      "Number of rows after: 93420\n"
     ]
    }
   ],
   "source": [
    "# Drop rows without zip_code since it is used for geocoding\n",
    "print(f'Number of rows before: {len(df)}')\n",
    "df.dropna(subset=['zip_code'], inplace=True)\n",
    "print(f'Number of rows after: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comma and any text that follows from address\n",
    "df.address = df.address.str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before: 93420\n",
      "Number of rows after: 92435\n"
     ]
    }
   ],
   "source": [
    "# Remove rows entries where address contains 'N/A'\n",
    "print(f'Number of rows before: {len(df)}')\n",
    "df = df[~df.address.str.contains('N/A')]\n",
    "print(f'Number of rows after: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip white outer white space from address column\n",
    "df.address = df.address.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into smaller chunks to prevent loss of progress\n",
    "n_chunks = 100\n",
    "data_chunks = np.array_split(df, n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress bar for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed chunk 1/100\n",
      "Already processed chunk 2/100\n",
      "Already processed chunk 3/100\n",
      "Already processed chunk 4/100\n",
      "Already processed chunk 5/100\n",
      "Already processed chunk 6/100\n",
      "Already processed chunk 7/100\n",
      "Already processed chunk 8/100\n",
      "Already processed chunk 9/100\n",
      "Already processed chunk 10/100\n",
      "Already processed chunk 11/100\n",
      "Already processed chunk 12/100\n",
      "Already processed chunk 13/100\n",
      "Already processed chunk 14/100\n",
      "Already processed chunk 15/100\n",
      "Already processed chunk 16/100\n",
      "Already processed chunk 17/100\n",
      "Already processed chunk 18/100\n",
      "Already processed chunk 19/100\n",
      "Already processed chunk 20/100\n",
      "Already processed chunk 21/100\n",
      "Already processed chunk 22/100\n",
      "Already processed chunk 23/100\n",
      "Already processed chunk 24/100\n",
      "Already processed chunk 25/100\n",
      "Already processed chunk 26/100\n",
      "Already processed chunk 27/100\n",
      "Already processed chunk 28/100\n",
      "Already processed chunk 29/100\n",
      "Already processed chunk 30/100\n",
      "Already processed chunk 31/100\n",
      "Already processed chunk 32/100\n",
      "Already processed chunk 33/100\n",
      "Already processed chunk 34/100\n",
      "Already processed chunk 35/100\n",
      "Already processed chunk 36/100\n",
      "Already processed chunk 37/100\n",
      "Already processed chunk 38/100\n",
      "Already processed chunk 39/100\n",
      "Already processed chunk 40/100\n",
      "Already processed chunk 41/100\n",
      "Already processed chunk 42/100\n",
      "Already processed chunk 43/100\n",
      "Already processed chunk 44/100\n",
      "Already processed chunk 45/100\n",
      "Already processed chunk 46/100\n",
      "Already processed chunk 47/100\n",
      "Already processed chunk 48/100\n",
      "Already processed chunk 49/100\n",
      "Already processed chunk 50/100\n",
      "Already processed chunk 51/100\n",
      "Already processed chunk 52/100\n",
      "Already processed chunk 53/100\n",
      "Already processed chunk 54/100\n",
      "Already processed chunk 55/100\n",
      "Already processed chunk 56/100\n",
      "Already processed chunk 57/100\n",
      "Already processed chunk 58/100\n",
      "Already processed chunk 59/100\n",
      "Already processed chunk 60/100\n",
      "Already processed chunk 61/100\n",
      "Already processed chunk 62/100\n",
      "Already processed chunk 63/100\n",
      "Already processed chunk 64/100\n",
      "Already processed chunk 65/100\n",
      "Already processed chunk 66/100\n",
      "Already processed chunk 67/100\n",
      "Already processed chunk 68/100\n",
      "Already processed chunk 69/100\n",
      "Already processed chunk 70/100\n",
      "Already processed chunk 71/100\n",
      "Already processed chunk 72/100\n",
      "Already processed chunk 73/100\n",
      "Already processed chunk 74/100\n",
      "Already processed chunk 75/100\n",
      "Already processed chunk 76/100\n",
      "Already processed chunk 77/100\n",
      "Already processed chunk 78/100\n",
      "Already processed chunk 79/100\n",
      "Already processed chunk 80/100\n",
      "Already processed chunk 81/100\n",
      "Already processed chunk 82/100\n",
      "Already processed chunk 83/100\n",
      "Already processed chunk 84/100\n",
      "Already processed chunk 85/100\n",
      "Already processed chunk 86/100\n",
      "Already processed chunk 87/100\n",
      "Already processed chunk 88/100\n",
      "Already processed chunk 89/100\n",
      "Already processed chunk 90/100\n",
      "Already processed chunk 91/100\n",
      "Already processed chunk 92/100\n",
      "Already processed chunk 93/100\n",
      "Already processed chunk 94/100\n",
      "Already processed chunk 95/100\n",
      "Already processed chunk 96/100\n",
      "Already processed chunk 97/100\n",
      "Already processed chunk 98/100\n",
      "Already processed chunk 99/100\n",
      "Already processed chunk 100/100\n"
     ]
    }
   ],
   "source": [
    "# Geocode all property sales records\n",
    "for i, data_chunk in enumerate(data_chunks):\n",
    "    if Path(f'{file_path}{file_name}_geocoded_chunks/{file_name}_geocoded_{i + 1}-{n_chunks}.parquet').is_file():\n",
    "        print(f'Already processed chunk {i + 1}/{n_chunks}')\n",
    "    else:\n",
    "        print(f'Processing chunk {i + 1}/{n_chunks}')\n",
    "        data_chunk[['location_lat', 'location_long']] = data_chunk.progress_apply(\n",
    "            lambda x: geocode({'street': x['address'], 'postalcode': x['zip_code'], 'country': 'US'}), axis=1).apply(\n",
    "            lambda x: pd.Series([np.nan, np.nan] if x is None else [x.latitude, x.longitude], index=['location_lat', 'location_long'])\n",
    "            )\n",
    "        \n",
    "        # Save progress\n",
    "        data_chunk.to_parquet(f'{file_path}{file_name}_geocoded_chunks/{file_name}_geocoded_{i + 1}-{n_chunks}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat individual data chunks\n",
    "data_chunks_geocoded = [pd.read_parquet(f'{file_path}{file_name}_geocoded_chunks/{file_name}_geocoded_{i + 1}-{n_chunks}.parquet') for i in range(len(data_chunks))]\n",
    "df = pd.concat(data_chunks_geocoded, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save geocoded data\n",
    "df.to_csv(f'{file_path}{file_name}_geocoded.csv')\n",
    "df.to_parquet(f'{file_path}{file_name}_geocoded.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelors_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
